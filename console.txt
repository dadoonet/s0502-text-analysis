# Introduction
## Look at some text fields 
GET bytes-discuss/_search?track_total_hits=true
{
  "_source": {
    "includes": [
      "title",
      "category_name",
      "question.text",
      "question.author",
      "solution.text",
      "solution.author"
    ]
  }
}

## Search for grokparsefailure
GET bytes-discuss/_search?track_total_hits=true
{
  "size": 30, 
  "_source": {
    "includes": [
      "title",
      "category_name",
      "question.text",
      "question.author",
      "solution.text",
      "solution.author"
    ]
  },
  "query": {
    "match": {
      "title": "grokparsefailure"
    }
  }
}

## Search for _grokparsefailure
GET bytes-discuss/_search?track_total_hits=true
{
  "size": 30, 
  "_source": {
    "includes": [
      "title",
      "category_name",
      "question.text",
      "question.author",
      "solution.text",
      "solution.author"
    ]
  },
  "query": {
    "match": {
      "title": "_grokparsefailure"
    }
  }
}

# title field analysis
## Analyze _grokparsefailure
POST bytes-discuss/_analyze
{
  "field": "title",
  "text": "_grokparsefailure"
}

## Analyze grokparsefailure
POST bytes-discuss/_analyze
{
  "field": "title",
  "text": "grokparsefailure"
}

# The standard analyzer
## Using the standard analyzer with 2 texts
POST _analyze
{
  "analyzer": "standard",
  "text": [ "_grokparsefailure PROBLEM", "grokparsefailure issue" ]
}

## The standard analyzer behind the scene
POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase"
  ],
  "text": [ "_grokparsefailure PROBLEM", "grokparsefailure issue" ]
}

# Choose the tokenizer
## Add char_group on whitespace and punctuation
POST _analyze
{
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "punctuation"
    ]
  },
  "filter": [
    "lowercase"
  ],
  "text": [ "_grokparsefailure PROBLEM", "grokparsefailure issue" ]
}

# With HTML elements
## It looks <b>BIG</b>
POST _analyze
{
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "punctuation"
    ]
  },
  "filter": [
    "lowercase"
  ],
  "text": "I have a <b>BIG</b> issue!"
}

## Add html_strip char_filter
POST _analyze
{
  "char_filter": [
    "html_strip"
  ],
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "punctuation"
    ]
  },
  "filter": [
    "lowercase"
  ],
  "text": "I have a <b>BIG</b> issue!"
}

## Explain the result, step by step
POST _analyze
{
  "explain": true,
  "char_filter": [
    "html_strip"
  ],
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "punctuation"
    ]
  },
  "filter": [
    "lowercase"
  ],
  "text": "I have a <b>BIG</b> issue!"
}

# Stemming
## Searching for issues
POST _analyze
{
  "char_filter": [
    "html_strip"
  ],
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "punctuation"
    ]
  },
  "filter": [
    "lowercase"
  ],
  "text": ["issue", "issues"]
}

# With an english stemmer
POST _analyze
{
  "char_filter": [
    "html_strip"
  ],
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "punctuation"
    ]
  },
  "filter": [
    "lowercase",
    {
      "type": "stemmer",
      "language": "english" 
    }
  ],
  "text": ["issue", "issues"]
}

# path analysis
## Analyze with field avatar_template
POST bytes-discuss/_analyze
{
  "field": "question.author.avatar_template",
  "text": "/user_avatar/discuss.elastic.co/dadoonet/{size}/44959_2.png"
}

## It's a keyword behind the scene
POST _analyze
{
  "tokenizer": "keyword",
  "text": "/user_avatar/discuss.elastic.co/dadoonet/{size}/44959_2.png"
}

## Using a path_hierarchy
POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/user_avatar/discuss.elastic.co/dadoonet/{size}/44959_2.png"
}

# Search as you type
## Search for dado
POST _analyze
{
  "analyzer": "simple",
  "text": ["dado"]
}

## Edge Ngrams
POST _analyze
{
  "tokenizer": {
    "type": "keyword"
  },
  "filter": [
    "lowercase",
    { 
      "type": "edge_ngram",
      "min_gram": 2,
      "max_gram": 5
    }
  ],
  "text": ["dadoonet"]
}

# Search for _grokparsefailure in bytes-discuss-02
## bytes-discuss-02 is the bytes-discuss with new analysis settings
GET bytes-discuss-02/_search
{
  "query": {
    "match": {
      "title": "_grokparsefailure"
    }
  }
}
